{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is code for \n",
    "1. running our fmri scans through an autoencoder\n",
    "2. encoding our fmri data from 4d scans to 1d arrays with a length of 1000\n",
    "3. predicting schizophrenia with the encoded scans\n",
    "\n",
    "right now the predictions are still no better than random chance. But this is a template for how to autoencoder could work. We still need to work on preprocessing the fmri data before it even gets to the autoencoder. The data is either too noisy or too big for the autoencoder to accuratly condense the information into something that can accuratly predict schizophrenia. One solution would also be to make the autoencoder way larger, but since the input layer has a vector length of over 3 million, even small increases make the autoencoder way larger quickly. This one has 6 billion total weight and bias parameters. Our best option at the moment would be to work on preprocessing before the autoencoder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from nibabel.testing import data_path\n",
    "import nibabel as nib\n",
    "import nilearn\n",
    "\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "%matplotlib inline\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load these data files, look at other files in repository,\n",
    "XData = np.load('/Users/KJP/Desktop/neural nets/Codes/XData.npy')\n",
    "YData = np.load('/Users/KJP/Desktop/neural nets/Codes/YData.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch dataset\n",
    "dataset = nilearn.datasets.fetch_cobre(n_subjects=146, data_dir=\"/Users/KJP/Desktop/neural nets/Cobre Dataset2\", url=None, verbose=1)\n",
    "file_paths = dataset[\"func\"]\n",
    "confounds = dataset[\"confounds\"]\n",
    "file_paths.sort() #sort file names by alphabetical order, which will result in sorting by patient number\n",
    "confounds.sort()\n",
    "del file_paths[74]                           # number 74 is misisng samples so it needs to be removed\n",
    "del confounds[74]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(145, 27, 32, 26, 150)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# #fmri scans, l, h, w, time stamps for each scan\n",
    "XData.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize XData into X_scaled\n",
    "X_scaled = XData.astype('float32').reshape(XData.shape[0],XData.shape[1]*XData.shape[2]*XData.shape[3]*XData.shape[4])\n",
    "X_scaled  = X_scaled - X_scaled.mean()\n",
    "X_scaled = X_scaled / X_scaled.max()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(115, 3369600)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(30, 3369600)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(115, 2)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(30, 2)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Split X_scaled and YData into testing and training\n",
    "x_train = X_scaled[:115]\n",
    "x_test = X_scaled[115:]\n",
    "y_train = YData[:115]\n",
    "y_test = YData[115:]\n",
    "display(x_train.shape)\n",
    "display(x_test.shape)\n",
    "display(y_train.shape)\n",
    "display(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_17 (Dense)             (None, 1000)              3369601000\n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 500)               500500    \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 200)               100200    \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 200)               40200     \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 200)               40200     \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 500)               100500    \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 1000)              501000    \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 3369600)           3372969600\n",
      "=================================================================\n",
      "Total params: 6,743,853,200\n",
      "Trainable params: 6,743,853,200\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Make autoencoder network\n",
    "input_dim = x_train.shape[1]\n",
    "encoding_dim = 200\n",
    "\n",
    "autoencoder = keras.Sequential()\n",
    "\n",
    "# Encoder Layers\n",
    "autoencoder.add(Dense(1000, input_shape=(input_dim,), activation='relu'))\n",
    "autoencoder.add(Dense(500, activation='relu'))\n",
    "autoencoder.add(Dense(200, activation='relu'))\n",
    "\n",
    "autoencoder.add(Dense(encoding_dim, activation='relu'))\n",
    "\n",
    "# Decoder Layers\n",
    "autoencoder.add(Dense(200, activation='relu'))\n",
    "autoencoder.add(Dense(500, activation='relu'))\n",
    "autoencoder.add(Dense(1000, activation='relu'))\n",
    "autoencoder.add(Dense(input_dim, activation='sigmoid'))\n",
    "\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 3369600)           0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 1000)              3369601000\n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 500)               500500    \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 200)               100200    \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 200)               40200     \n",
      "=================================================================\n",
      "Total params: 3,370,241,900\n",
      "Trainable params: 3,370,241,900\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Make seperate encoder network from the autoencoder\n",
    "input_img = Input(shape=(input_dim,))\n",
    "encoder_layer1 = autoencoder.layers[0]\n",
    "encoder_layer2 = autoencoder.layers[1]\n",
    "encoder_layer3 = autoencoder.layers[2]\n",
    "encoder_layer4 = autoencoder.layers[3]\n",
    "\n",
    "encoder = Model(input_img, encoder_layer4(encoder_layer3(encoder_layer2(encoder_layer1(input_img)))))\n",
    "\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=.0003, patience=10)\n",
    "\n",
    "autoencoder.compile(optimizer=keras.optimizers.Adam(lr=.0005), loss='mean_squared_error')\n",
    "history = autoencoder.fit(x_train[10:20, :], x_train[10:20, :],\n",
    "                epochs=100,\n",
    "                batch_size=10,\n",
    "                validation_data=(x_test[:15,:], x_test[:15,:]),\n",
    "                verbose = 1,\n",
    "                callbacks=[early_stopping])\n",
    "plt.figure()\n",
    "# summarize history for accuracy\n",
    "plt.subplot(211)\n",
    "#plt.plot(history.history['acc'])\n",
    "#plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "# summarize history for loss\n",
    "plt.subplot(212)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "score = autoencoder.evaluate(x_test, x_test, verbose=0)\n",
    "print('Test loss:', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode all of our x data from a 4d fmri to 1000 length 1d array\n",
    "X_encoded = encoder.predict(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize X\n",
    "X_encoded  = X_encoded - X_encoded.mean()\n",
    "X_encoded = X_encoded / X_encoded.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(115, 1000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(30, 1000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_train_encoded = X_encoded[:115, :]\n",
    "x_test_encoded = X_encoded[115:, :]\n",
    "display(x_train_encoded.shape)\n",
    "display(x_test_encoded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train with X_encoded on a neural net\n",
    "\n",
    "# Multi-layer net with ReLU hidden layer\n",
    "model = keras.models.Sequential()\n",
    "# Here we make the hidden layer (size 2) with a ReL\n",
    "# activation function, but also initialize the bias\n",
    "# weights in the network to a constant 0.1\n",
    "model.add(keras.layers.Dense(10000,input_dim=len(X_encoded[0]),activation='relu',bias_initializer=keras.initializers.Constant(0.1)))\n",
    "model.add(keras.layers.Dense(1000,activation='relu',bias_initializer=keras.initializers.Constant(0.1)))\n",
    "model.add(keras.layers.Dense(100,activation='relu',bias_initializer=keras.initializers.Constant(0.1)))\n",
    "# Output layer (size 1), sigmoid activation function\n",
    "model.add(keras.layers.Dense(len(YData[0]),activation='softmax'))\n",
    "# Compile as above (default learning rate and other\n",
    "# hyperparameters for the Adam optimizer).\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "             optimizer=keras.optimizers.Adam(lr=.0001),\n",
    "             metrics=['accuracy'])\n",
    "# Display the model\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 92 samples, validate on 23 samples\n",
      "Epoch 1/200\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.7119 - acc: 0.5435 - val_loss: 2.2129 - val_acc: 0.3913\n",
      "Epoch 2/200\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 1.9944 - acc: 0.4565 - val_loss: 1.1672 - val_acc: 0.3913\n",
      "Epoch 3/200\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 1.0650 - acc: 0.4565 - val_loss: 0.6790 - val_acc: 0.6087\n",
      "Epoch 4/200\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 0.7211 - acc: 0.5435 - val_loss: 0.7080 - val_acc: 0.6087\n",
      "Epoch 5/200\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 0.7733 - acc: 0.5435 - val_loss: 0.6858 - val_acc: 0.6087\n",
      "Epoch 6/200\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 0.7372 - acc: 0.5435 - val_loss: 0.6703 - val_acc: 0.6087\n",
      "Epoch 7/200\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 0.6973 - acc: 0.5435 - val_loss: 0.6908 - val_acc: 0.6087\n",
      "Epoch 8/200\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 0.6924 - acc: 0.5435 - val_loss: 0.7267 - val_acc: 0.3913\n",
      "Epoch 9/200\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 0.7117 - acc: 0.4565 - val_loss: 0.7360 - val_acc: 0.3913\n",
      "Epoch 10/200\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 0.7178 - acc: 0.4565 - val_loss: 0.7161 - val_acc: 0.3913\n",
      "Epoch 11/200\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 0.7053 - acc: 0.4565 - val_loss: 0.6880 - val_acc: 0.6087\n",
      "Epoch 12/200\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 0.6914 - acc: 0.5435 - val_loss: 0.6720 - val_acc: 0.6087\n",
      "Epoch 13/200\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 0.6908 - acc: 0.5435 - val_loss: 0.6695 - val_acc: 0.6087\n",
      "Epoch 14/200\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 0.6998 - acc: 0.5435 - val_loss: 0.6703 - val_acc: 0.6087\n",
      "Epoch 15/200\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 0.7045 - acc: 0.5435 - val_loss: 0.6690 - val_acc: 0.6087\n",
      "Epoch 16/200\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 0.6996 - acc: 0.5435 - val_loss: 0.6700 - val_acc: 0.6087\n",
      "Epoch 17/200\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 0.6915 - acc: 0.5435 - val_loss: 0.6785 - val_acc: 0.6087\n",
      "Epoch 18/200\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 0.6890 - acc: 0.5435 - val_loss: 0.6915 - val_acc: 0.6087\n",
      "Epoch 19/200\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 0.6932 - acc: 0.5109 - val_loss: 0.6995 - val_acc: 0.3913\n",
      "Epoch 20/200\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 0.6970 - acc: 0.4565 - val_loss: 0.6965 - val_acc: 0.3913\n",
      "Epoch 21/200\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 0.6955 - acc: 0.4565 - val_loss: 0.6857 - val_acc: 0.6087\n",
      "Epoch 22/200\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 0.6909 - acc: 0.5435 - val_loss: 0.6751 - val_acc: 0.6087\n",
      "Epoch 23/200\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 0.6886 - acc: 0.5435 - val_loss: 0.6693 - val_acc: 0.6087\n",
      "Epoch 24/200\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 0.6905 - acc: 0.5435 - val_loss: 0.6677 - val_acc: 0.6087\n",
      "Epoch 25/200\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 0.6931 - acc: 0.5435 - val_loss: 0.6676 - val_acc: 0.6087\n",
      "Epoch 26/200\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 0.6927 - acc: 0.5435 - val_loss: 0.6688 - val_acc: 0.6087\n",
      "Epoch 27/200\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 0.6901 - acc: 0.5435 - val_loss: 0.6730 - val_acc: 0.6087\n",
      "Epoch 28/200\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 0.6883 - acc: 0.5435 - val_loss: 0.6793 - val_acc: 0.6087\n",
      "Epoch 29/200\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 0.6890 - acc: 0.5435 - val_loss: 0.6841 - val_acc: 0.6522\n",
      "Epoch 30/200\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 0.6906 - acc: 0.5978 - val_loss: 0.6837 - val_acc: 0.6522\n",
      "Epoch 31/200\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 0.6906 - acc: 0.6087 - val_loss: 0.6787 - val_acc: 0.6087\n",
      "Epoch 32/200\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 0.6889 - acc: 0.5435 - val_loss: 0.6726 - val_acc: 0.6087\n",
      "Epoch 33/200\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 0.6878 - acc: 0.5435 - val_loss: 0.6683 - val_acc: 0.6087\n",
      "Epoch 34/200\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 0.6883 - acc: 0.5435 - val_loss: 0.6664 - val_acc: 0.6087\n",
      "Epoch 35/200\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 0.6893 - acc: 0.5435 - val_loss: 0.6660 - val_acc: 0.6087\n",
      "Epoch 36/200\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 0.6891 - acc: 0.5435 - val_loss: 0.6677 - val_acc: 0.6087\n",
      "Epoch 37/200\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 0.6879 - acc: 0.5435 - val_loss: 0.6710 - val_acc: 0.6087\n",
      "Epoch 38/200\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 0.6874 - acc: 0.5435 - val_loss: 0.6746 - val_acc: 0.6087\n",
      "Epoch 39/200\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 0.6879 - acc: 0.5435 - val_loss: 0.6760 - val_acc: 0.6087\n",
      "Epoch 40/200\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 0.6883 - acc: 0.5543 - val_loss: 0.6742 - val_acc: 0.6087\n",
      "Epoch 41/200\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 0.6878 - acc: 0.5435 - val_loss: 0.6705 - val_acc: 0.6087\n",
      "Epoch 42/200\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 0.6871 - acc: 0.5435 - val_loss: 0.6670 - val_acc: 0.6087\n",
      "Epoch 43/200\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 0.6871 - acc: 0.5435 - val_loss: 0.6650 - val_acc: 0.6087\n",
      "Epoch 44/200\n",
      "92/92 [==============================] - 0s 4ms/step - loss: 0.6874 - acc: 0.5435 - val_loss: 0.6645 - val_acc: 0.6087\n",
      "Epoch 45/200\n",
      "92/92 [==============================] - 0s 4ms/step - loss: 0.6874 - acc: 0.5435 - val_loss: 0.6654 - val_acc: 0.6087\n",
      "Epoch 46/200\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 0.6870 - acc: 0.5435 - val_loss: 0.6673 - val_acc: 0.6087\n",
      "Epoch 47/200\n",
      "92/92 [==============================] - 0s 4ms/step - loss: 0.6866 - acc: 0.5435 - val_loss: 0.6694 - val_acc: 0.6087\n",
      "Epoch 48/200\n",
      "92/92 [==============================] - 0s 4ms/step - loss: 0.6868 - acc: 0.5435 - val_loss: 0.6703 - val_acc: 0.6087\n",
      "Epoch 49/200\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 0.6869 - acc: 0.5435 - val_loss: 0.6691 - val_acc: 0.6087\n",
      "Epoch 50/200\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 0.6867 - acc: 0.5435 - val_loss: 0.6667 - val_acc: 0.6087\n",
      "Epoch 51/200\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 0.6863 - acc: 0.5435 - val_loss: 0.6644 - val_acc: 0.6087\n",
      "Epoch 52/200\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 0.6863 - acc: 0.5435 - val_loss: 0.6629 - val_acc: 0.6087\n",
      "Epoch 53/200\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 0.6864 - acc: 0.5435 - val_loss: 0.6626 - val_acc: 0.6087\n",
      "Epoch 54/200\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 0.6863 - acc: 0.5435 - val_loss: 0.6633 - val_acc: 0.6087\n",
      "Epoch 55/200\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 0.6860 - acc: 0.5435 - val_loss: 0.6646 - val_acc: 0.6087\n",
      "Epoch 56/200\n",
      "92/92 [==============================] - 0s 4ms/step - loss: 0.6859 - acc: 0.5435 - val_loss: 0.6656 - val_acc: 0.6087\n",
      "Epoch 57/200\n",
      "92/92 [==============================] - 0s 4ms/step - loss: 0.6860 - acc: 0.5435 - val_loss: 0.6656 - val_acc: 0.6087\n",
      "Epoch 58/200\n",
      "92/92 [==============================] - 0s 4ms/step - loss: 0.6859 - acc: 0.5543 - val_loss: 0.6643 - val_acc: 0.6087\n",
      "Epoch 59/200\n",
      "92/92 [==============================] - 0s 4ms/step - loss: 0.6857 - acc: 0.5435 - val_loss: 0.6625 - val_acc: 0.6087\n",
      "Epoch 60/200\n",
      "92/92 [==============================] - 0s 4ms/step - loss: 0.6856 - acc: 0.5435 - val_loss: 0.6611 - val_acc: 0.6087\n",
      "Epoch 61/200\n",
      "92/92 [==============================] - 0s 4ms/step - loss: 0.6856 - acc: 0.5435 - val_loss: 0.6604 - val_acc: 0.6087\n",
      "Epoch 62/200\n",
      "92/92 [==============================] - 0s 4ms/step - loss: 0.6855 - acc: 0.5435 - val_loss: 0.6605 - val_acc: 0.6087\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63/200\n",
      "92/92 [==============================] - 0s 4ms/step - loss: 0.6854 - acc: 0.5435 - val_loss: 0.6612 - val_acc: 0.6087\n",
      "Epoch 64/200\n",
      "92/92 [==============================] - 0s 4ms/step - loss: 0.6852 - acc: 0.5435 - val_loss: 0.6618 - val_acc: 0.6087\n",
      "Epoch 65/200\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 0.6852 - acc: 0.5543 - val_loss: 0.6618 - val_acc: 0.6087\n",
      "Epoch 66/200\n",
      "92/92 [==============================] - 0s 4ms/step - loss: 0.6852 - acc: 0.5543 - val_loss: 0.6610 - val_acc: 0.6087\n",
      "Epoch 67/200\n",
      "92/92 [==============================] - 0s 4ms/step - loss: 0.6850 - acc: 0.5543 - val_loss: 0.6597 - val_acc: 0.6087\n",
      "Epoch 68/200\n",
      "92/92 [==============================] - 0s 4ms/step - loss: 0.6849 - acc: 0.5435 - val_loss: 0.6586 - val_acc: 0.6087\n",
      "Epoch 69/200\n",
      "92/92 [==============================] - 0s 4ms/step - loss: 0.6849 - acc: 0.5435 - val_loss: 0.6580 - val_acc: 0.6087\n",
      "Epoch 70/200\n",
      "92/92 [==============================] - 0s 4ms/step - loss: 0.6848 - acc: 0.5435 - val_loss: 0.6580 - val_acc: 0.6087\n",
      "Epoch 71/200\n",
      "92/92 [==============================] - 0s 4ms/step - loss: 0.6847 - acc: 0.5435 - val_loss: 0.6583 - val_acc: 0.6087\n",
      "Epoch 72/200\n",
      "92/92 [==============================] - 1s 5ms/step - loss: 0.6846 - acc: 0.5543 - val_loss: 0.6585 - val_acc: 0.6087\n",
      "Epoch 73/200\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 0.6846 - acc: 0.5761 - val_loss: 0.6583 - val_acc: 0.6087\n",
      "Epoch 74/200\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 0.6845 - acc: 0.5870 - val_loss: 0.6575 - val_acc: 0.6087\n",
      "Epoch 75/200\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 0.6844 - acc: 0.5761 - val_loss: 0.6566 - val_acc: 0.6087\n",
      "Epoch 76/200\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 0.6843 - acc: 0.5652 - val_loss: 0.6558 - val_acc: 0.6087\n",
      "Epoch 77/200\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 0.6842 - acc: 0.5543 - val_loss: 0.6554 - val_acc: 0.6087\n",
      "Epoch 78/200\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 0.6842 - acc: 0.5543 - val_loss: 0.6553 - val_acc: 0.6087\n",
      "Epoch 79/200\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 0.6841 - acc: 0.5652 - val_loss: 0.6554 - val_acc: 0.6087\n",
      "Epoch 80/200\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 0.6840 - acc: 0.6087 - val_loss: 0.6553 - val_acc: 0.6087\n",
      "Epoch 81/200\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 0.6839 - acc: 0.6087 - val_loss: 0.6549 - val_acc: 0.6087\n",
      "Epoch 82/200\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 0.6839 - acc: 0.5978 - val_loss: 0.6541 - val_acc: 0.6087\n",
      "Epoch 83/200\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 0.6838 - acc: 0.6087 - val_loss: 0.6534 - val_acc: 0.6087\n",
      "Epoch 84/200\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 0.6837 - acc: 0.6087 - val_loss: 0.6529 - val_acc: 0.6087\n",
      "Epoch 85/200\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 0.6836 - acc: 0.6087 - val_loss: 0.6526 - val_acc: 0.6087\n",
      "Epoch 86/200\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 0.6836 - acc: 0.6087 - val_loss: 0.6526 - val_acc: 0.6087\n",
      "Epoch 87/200\n",
      "92/92 [==============================] - 1s 8ms/step - loss: 0.6835 - acc: 0.5978 - val_loss: 0.6524 - val_acc: 0.6087\n",
      "Epoch 88/200\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 0.6834 - acc: 0.5978 - val_loss: 0.6520 - val_acc: 0.6087\n",
      "Epoch 89/200\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 0.6833 - acc: 0.5978 - val_loss: 0.6514 - val_acc: 0.6087\n",
      "Epoch 90/200\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 0.6833 - acc: 0.5978 - val_loss: 0.6508 - val_acc: 0.6087\n",
      "Epoch 91/200\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 0.6832 - acc: 0.5978 - val_loss: 0.6503 - val_acc: 0.6087\n",
      "Epoch 92/200\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 0.6831 - acc: 0.5978 - val_loss: 0.6500 - val_acc: 0.6087\n",
      "Epoch 93/200\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 0.6831 - acc: 0.5978 - val_loss: 0.6498 - val_acc: 0.6087\n",
      "Epoch 94/200\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 0.6830 - acc: 0.6196 - val_loss: 0.6496 - val_acc: 0.6087\n",
      "Epoch 95/200\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 0.6829 - acc: 0.6196 - val_loss: 0.6492 - val_acc: 0.6087\n",
      "Epoch 96/200\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 0.6828 - acc: 0.6196 - val_loss: 0.6487 - val_acc: 0.6087\n",
      "Epoch 97/200\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 0.6828 - acc: 0.6196 - val_loss: 0.6481 - val_acc: 0.6087\n",
      "Epoch 98/200\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 0.6827 - acc: 0.6196 - val_loss: 0.6477 - val_acc: 0.6087\n",
      "Epoch 99/200\n",
      "92/92 [==============================] - 1s 8ms/step - loss: 0.6826 - acc: 0.6196 - val_loss: 0.6474 - val_acc: 0.6087\n",
      "Epoch 100/200\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 0.6826 - acc: 0.6196 - val_loss: 0.6471 - val_acc: 0.6087\n",
      "Epoch 101/200\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 0.6825 - acc: 0.6087 - val_loss: 0.6468 - val_acc: 0.6087\n",
      "Epoch 102/200\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 0.6824 - acc: 0.6087 - val_loss: 0.6463 - val_acc: 0.6087\n",
      "Epoch 103/200\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 0.6824 - acc: 0.6087 - val_loss: 0.6459 - val_acc: 0.6087\n",
      "Epoch 104/200\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 0.6823 - acc: 0.6087 - val_loss: 0.6454 - val_acc: 0.6087\n",
      "Epoch 105/200\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 0.6822 - acc: 0.6087 - val_loss: 0.6450 - val_acc: 0.6522\n",
      "Epoch 106/200\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 0.6822 - acc: 0.6087 - val_loss: 0.6447 - val_acc: 0.6522\n",
      "Epoch 107/200\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 0.6821 - acc: 0.6087 - val_loss: 0.6444 - val_acc: 0.6522\n",
      "Epoch 108/200\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 0.6821 - acc: 0.5978 - val_loss: 0.6440 - val_acc: 0.6522\n",
      "Epoch 109/200\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 0.6820 - acc: 0.5978 - val_loss: 0.6435 - val_acc: 0.6522\n",
      "Epoch 110/200\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 0.6819 - acc: 0.5978 - val_loss: 0.6431 - val_acc: 0.6522\n",
      "Epoch 111/200\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 0.6819 - acc: 0.5978 - val_loss: 0.6427 - val_acc: 0.6522\n",
      "Epoch 112/200\n",
      "92/92 [==============================] - 1s 10ms/step - loss: 0.6818 - acc: 0.5978 - val_loss: 0.6423 - val_acc: 0.6522\n",
      "Epoch 113/200\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 0.6818 - acc: 0.5978 - val_loss: 0.6420 - val_acc: 0.6522\n",
      "Epoch 114/200\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 0.6817 - acc: 0.5978 - val_loss: 0.6416 - val_acc: 0.6522\n",
      "Epoch 115/200\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 0.6817 - acc: 0.6087 - val_loss: 0.6412 - val_acc: 0.6522\n",
      "Epoch 116/200\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 0.6816 - acc: 0.6087 - val_loss: 0.6408 - val_acc: 0.6522\n",
      "Epoch 117/200\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 0.6815 - acc: 0.6087 - val_loss: 0.6404 - val_acc: 0.6522\n",
      "Epoch 118/200\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 0.6815 - acc: 0.5978 - val_loss: 0.6401 - val_acc: 0.6522\n",
      "Epoch 119/200\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 0.6814 - acc: 0.5978 - val_loss: 0.6397 - val_acc: 0.6522\n",
      "Epoch 120/200\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 0.6814 - acc: 0.6087 - val_loss: 0.6394 - val_acc: 0.6522\n",
      "Epoch 121/200\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 0.6813 - acc: 0.6087 - val_loss: 0.6390 - val_acc: 0.6522\n",
      "Epoch 122/200\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 0.6813 - acc: 0.6087 - val_loss: 0.6386 - val_acc: 0.6522\n",
      "Epoch 123/200\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 0.6812 - acc: 0.6087 - val_loss: 0.6382 - val_acc: 0.6522\n",
      "Epoch 124/200\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 0.6812 - acc: 0.6087 - val_loss: 0.6378 - val_acc: 0.6522\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 125/200\n",
      "92/92 [==============================] - 0s 4ms/step - loss: 0.6811 - acc: 0.5978 - val_loss: 0.6375 - val_acc: 0.6522\n",
      "Epoch 126/200\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 0.6811 - acc: 0.5978 - val_loss: 0.6371 - val_acc: 0.6522\n",
      "Epoch 127/200\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 0.6811 - acc: 0.5978 - val_loss: 0.6367 - val_acc: 0.6522\n",
      "Epoch 128/200\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 0.6810 - acc: 0.5978 - val_loss: 0.6363 - val_acc: 0.6522\n",
      "Epoch 129/200\n",
      "92/92 [==============================] - 0s 4ms/step - loss: 0.6810 - acc: 0.5978 - val_loss: 0.6360 - val_acc: 0.6522\n",
      "Epoch 130/200\n",
      "92/92 [==============================] - 0s 4ms/step - loss: 0.6809 - acc: 0.5870 - val_loss: 0.6356 - val_acc: 0.6522\n",
      "Epoch 131/200\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 0.6809 - acc: 0.5870 - val_loss: 0.6353 - val_acc: 0.6522\n",
      "Epoch 132/200\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 0.6808 - acc: 0.5870 - val_loss: 0.6349 - val_acc: 0.6522\n",
      "Epoch 133/200\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 0.6808 - acc: 0.5870 - val_loss: 0.6345 - val_acc: 0.6522\n",
      "Epoch 134/200\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 0.6808 - acc: 0.5870 - val_loss: 0.6342 - val_acc: 0.6522\n",
      "Epoch 135/200\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 0.6807 - acc: 0.5870 - val_loss: 0.6339 - val_acc: 0.6522\n",
      "Epoch 136/200\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 0.6807 - acc: 0.5870 - val_loss: 0.6335 - val_acc: 0.6522\n",
      "Epoch 137/200\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 0.6807 - acc: 0.5870 - val_loss: 0.6332 - val_acc: 0.6522\n",
      "Epoch 138/200\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 0.6806 - acc: 0.5870 - val_loss: 0.6328 - val_acc: 0.6522\n",
      "Epoch 139/200\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 0.6806 - acc: 0.5870 - val_loss: 0.6325 - val_acc: 0.6522\n",
      "Epoch 140/200\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 0.6806 - acc: 0.5870 - val_loss: 0.6321 - val_acc: 0.6522\n",
      "Epoch 141/200\n",
      "92/92 [==============================] - 0s 4ms/step - loss: 0.6805 - acc: 0.5870 - val_loss: 0.6318 - val_acc: 0.6522\n",
      "Epoch 142/200\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 0.6805 - acc: 0.5870 - val_loss: 0.6315 - val_acc: 0.6522\n",
      "Epoch 143/200\n",
      "92/92 [==============================] - 1s 5ms/step - loss: 0.6805 - acc: 0.5870 - val_loss: 0.6311 - val_acc: 0.6522\n",
      "Epoch 144/200\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 0.6804 - acc: 0.5870 - val_loss: 0.6308 - val_acc: 0.6522\n",
      "Epoch 145/200\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 0.6804 - acc: 0.5870 - val_loss: 0.6304 - val_acc: 0.6522\n",
      "Epoch 146/200\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 0.6804 - acc: 0.5761 - val_loss: 0.6301 - val_acc: 0.6522\n",
      "Epoch 147/200\n",
      "92/92 [==============================] - 1s 8ms/step - loss: 0.6803 - acc: 0.5761 - val_loss: 0.6298 - val_acc: 0.6522\n",
      "Epoch 148/200\n",
      "92/92 [==============================] - 1s 9ms/step - loss: 0.6803 - acc: 0.5761 - val_loss: 0.6295 - val_acc: 0.6522\n",
      "Epoch 149/200\n",
      "92/92 [==============================] - 1s 10ms/step - loss: 0.6803 - acc: 0.5761 - val_loss: 0.6292 - val_acc: 0.6522\n",
      "Epoch 150/200\n",
      "92/92 [==============================] - 1s 8ms/step - loss: 0.6803 - acc: 0.5761 - val_loss: 0.6289 - val_acc: 0.6522\n",
      "Epoch 151/200\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 0.6802 - acc: 0.5761 - val_loss: 0.6286 - val_acc: 0.6522\n",
      "Epoch 152/200\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 0.6802 - acc: 0.5652 - val_loss: 0.6283 - val_acc: 0.6522\n",
      "Epoch 153/200\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 0.6802 - acc: 0.5543 - val_loss: 0.6280 - val_acc: 0.6522\n",
      "Epoch 154/200\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 0.6802 - acc: 0.5435 - val_loss: 0.6277 - val_acc: 0.6087\n",
      "Epoch 155/200\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 0.6801 - acc: 0.5326 - val_loss: 0.6273 - val_acc: 0.6087\n",
      "Epoch 156/200\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 0.6801 - acc: 0.5326 - val_loss: 0.6270 - val_acc: 0.6087\n",
      "Epoch 157/200\n",
      "92/92 [==============================] - 1s 8ms/step - loss: 0.6801 - acc: 0.5326 - val_loss: 0.6267 - val_acc: 0.6087\n",
      "Epoch 158/200\n",
      "92/92 [==============================] - 1s 8ms/step - loss: 0.6801 - acc: 0.5326 - val_loss: 0.6265 - val_acc: 0.6087\n",
      "Epoch 159/200\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 0.6801 - acc: 0.5326 - val_loss: 0.6262 - val_acc: 0.6087\n",
      "Epoch 160/200\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 0.6801 - acc: 0.5326 - val_loss: 0.6259 - val_acc: 0.6087\n",
      "Epoch 161/200\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 0.6800 - acc: 0.5326 - val_loss: 0.6256 - val_acc: 0.6087\n",
      "Epoch 162/200\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 0.6800 - acc: 0.5326 - val_loss: 0.6253 - val_acc: 0.6087\n",
      "Epoch 163/200\n",
      "92/92 [==============================] - 1s 8ms/step - loss: 0.6800 - acc: 0.5326 - val_loss: 0.6251 - val_acc: 0.6087\n",
      "Epoch 164/200\n",
      "92/92 [==============================] - 1s 9ms/step - loss: 0.6800 - acc: 0.5326 - val_loss: 0.6248 - val_acc: 0.6087\n",
      "Epoch 165/200\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 0.6800 - acc: 0.5326 - val_loss: 0.6246 - val_acc: 0.6087\n",
      "Epoch 166/200\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 0.6800 - acc: 0.5435 - val_loss: 0.6244 - val_acc: 0.6087\n",
      "Epoch 167/200\n",
      "92/92 [==============================] - 1s 8ms/step - loss: 0.6799 - acc: 0.5435 - val_loss: 0.6241 - val_acc: 0.6087\n",
      "Epoch 168/200\n",
      "92/92 [==============================] - 1s 10ms/step - loss: 0.6799 - acc: 0.5435 - val_loss: 0.6239 - val_acc: 0.6087\n",
      "Epoch 169/200\n",
      "92/92 [==============================] - 1s 8ms/step - loss: 0.6799 - acc: 0.5435 - val_loss: 0.6236 - val_acc: 0.6087\n",
      "Epoch 170/200\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 0.6799 - acc: 0.5543 - val_loss: 0.6234 - val_acc: 0.6087\n",
      "Epoch 171/200\n",
      "92/92 [==============================] - 1s 9ms/step - loss: 0.6799 - acc: 0.5543 - val_loss: 0.6232 - val_acc: 0.6087\n",
      "Epoch 172/200\n",
      "92/92 [==============================] - 1s 8ms/step - loss: 0.6799 - acc: 0.5543 - val_loss: 0.6230 - val_acc: 0.6087\n",
      "Epoch 173/200\n",
      "92/92 [==============================] - 1s 8ms/step - loss: 0.6799 - acc: 0.5543 - val_loss: 0.6228 - val_acc: 0.6087\n",
      "Epoch 174/200\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 0.6799 - acc: 0.5543 - val_loss: 0.6225 - val_acc: 0.6087\n",
      "Epoch 175/200\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 0.6799 - acc: 0.5543 - val_loss: 0.6224 - val_acc: 0.6087\n",
      "Epoch 176/200\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 0.6799 - acc: 0.5543 - val_loss: 0.6222 - val_acc: 0.6087\n",
      "Epoch 177/200\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 0.6799 - acc: 0.5543 - val_loss: 0.6220 - val_acc: 0.6087\n",
      "Epoch 178/200\n",
      "92/92 [==============================] - 0s 5ms/step - loss: 0.6798 - acc: 0.5543 - val_loss: 0.6219 - val_acc: 0.6087\n",
      "Epoch 179/200\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 0.6798 - acc: 0.5543 - val_loss: 0.6216 - val_acc: 0.6087\n",
      "Epoch 180/200\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 0.6798 - acc: 0.5543 - val_loss: 0.6214 - val_acc: 0.6087\n",
      "Epoch 181/200\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 0.6798 - acc: 0.5543 - val_loss: 0.6213 - val_acc: 0.6087\n",
      "Epoch 182/200\n",
      "92/92 [==============================] - 1s 10ms/step - loss: 0.6798 - acc: 0.5543 - val_loss: 0.6212 - val_acc: 0.6087\n",
      "Epoch 183/200\n",
      "92/92 [==============================] - 1s 8ms/step - loss: 0.6798 - acc: 0.5543 - val_loss: 0.6210 - val_acc: 0.6087\n",
      "Epoch 184/200\n",
      "92/92 [==============================] - 1s 9ms/step - loss: 0.6798 - acc: 0.5543 - val_loss: 0.6209 - val_acc: 0.6087\n",
      "Epoch 185/200\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 0.6798 - acc: 0.5543 - val_loss: 0.6207 - val_acc: 0.6087\n",
      "Epoch 186/200\n",
      "92/92 [==============================] - 1s 9ms/step - loss: 0.6798 - acc: 0.5543 - val_loss: 0.6206 - val_acc: 0.6087\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 187/200\n",
      "92/92 [==============================] - 1s 8ms/step - loss: 0.6798 - acc: 0.5543 - val_loss: 0.6204 - val_acc: 0.6087\n",
      "Epoch 188/200\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 0.6798 - acc: 0.5543 - val_loss: 0.6204 - val_acc: 0.6087\n",
      "Epoch 189/200\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 0.6798 - acc: 0.5543 - val_loss: 0.6202 - val_acc: 0.6087\n",
      "Epoch 190/200\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 0.6798 - acc: 0.5543 - val_loss: 0.6201 - val_acc: 0.6087\n",
      "Epoch 191/200\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 0.6798 - acc: 0.5543 - val_loss: 0.6200 - val_acc: 0.6087\n",
      "Epoch 192/200\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 0.6798 - acc: 0.5543 - val_loss: 0.6199 - val_acc: 0.6087\n",
      "Epoch 193/200\n",
      "92/92 [==============================] - 1s 7ms/step - loss: 0.6798 - acc: 0.5543 - val_loss: 0.6198 - val_acc: 0.6087\n",
      "Epoch 194/200\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 0.6798 - acc: 0.5543 - val_loss: 0.6198 - val_acc: 0.6087\n",
      "Epoch 195/200\n",
      "92/92 [==============================] - 1s 8ms/step - loss: 0.6798 - acc: 0.5543 - val_loss: 0.6197 - val_acc: 0.6087\n",
      "Epoch 196/200\n",
      "92/92 [==============================] - 1s 8ms/step - loss: 0.6798 - acc: 0.5543 - val_loss: 0.6196 - val_acc: 0.6087\n",
      "Epoch 197/200\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 0.6798 - acc: 0.5543 - val_loss: 0.6195 - val_acc: 0.6087\n",
      "Epoch 198/200\n",
      "92/92 [==============================] - 1s 6ms/step - loss: 0.6798 - acc: 0.5543 - val_loss: 0.6194 - val_acc: 0.6087\n",
      "Epoch 199/200\n",
      "92/92 [==============================] - 1s 8ms/step - loss: 0.6798 - acc: 0.5543 - val_loss: 0.6194 - val_acc: 0.6087\n",
      "Epoch 200/200\n",
      "92/92 [==============================] - 1s 8ms/step - loss: 0.6798 - acc: 0.5543 - val_loss: 0.6194 - val_acc: 0.6087\n"
     ]
    }
   ],
   "source": [
    "# Train it!\n",
    "history = model.fit(x_train_encoded, y_train,\n",
    "batch_size=100,\n",
    "epochs=200,\n",
    "verbose=1,\n",
    "validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "# summarize history for accuracy\n",
    "plt.subplot(211)\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "# summarize history for loss\n",
    "plt.subplot(212)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.743468165397644\n",
      "Test accuracy: 0.4333333373069763\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test_encoded, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Overall we still have not achieved better than random chance. The problem at the moment \n",
    "#is that we still need more preprocessing before the autoencoder\n",
    "#Either the data is too noisy or too big for the autoencoder to accurately encode useful\n",
    "#data for predicting schizophrenia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
